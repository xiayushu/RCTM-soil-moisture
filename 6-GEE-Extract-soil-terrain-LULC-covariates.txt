// Import public data sources for terrain factors and NLCD
// Soilgrid+ dataset for soil properties was downloaded and processed from https://scholarsphere.psu.edu/resources/ea4b6c45-9eba-4b89-aba6-ff7246880fb1
// NLCD2019 was downloaded and uploaded as an asset for this project but can now be accessed from https://developers.google.com/earth-engine/datasets/catalog/USGS_NLCD_RELEASES_2019_REL_NLCD
// Comment out sections to avoid computation time-out; If the file is still too large for GEE, then move the codes to Google Colab for processing

var SRTM = ee.Image("USGS/SRTMGL1_003"),
    flowaccum = ee.Image("WWF/HydroSHEDS/15ACC"),
    dem = ee.Image("WWF/HydroSHEDS/03VFDEM"),
    roughness = ee.ImageCollection("projects/sat-io/open-datasets/Geomorpho90m/roughness"),
    NLCD2019 = ee.Image("customized_pathway"),
    NLCD = ee.ImageCollection("USGS/NLCD_RELEASES/2016_REL"),
    SoilGrid100 = ee.ImageCollection("customized_pathway"),
    site_depth = ee.FeatureCollection("users/yxia/Moisture/site_depth"),
    site_year = ee.FeatureCollection("users/yxia/Moisture/site_year"),
    station = ee.FeatureCollection("users/yxia/Moisture/Station");


/* -------------Buffer function-------------*/ 

//Copy and paste GEE functions for creating point buffers for zonal statistics analysis
function bufferPoints(radius, bounds) {
  return function(pt) {
    pt = ee.Feature(pt);
    return bounds ? pt.buffer(radius).bounds() : pt.buffer(radius);
  };
}
function zonalStats(ic, fc, params) {
  var _params = {
    reducer: ee.Reducer.mean(),
    scale: null,
    crs: null,
    bands: null,
    bandsRename: null,
    imgProps: null,
    imgPropsRename: null,
    datetimeName: 'datetime',
    datetimeFormat: 'YYYY-MM-dd HH:mm:ss'
  };
  if (params) {
    for (var param in params) {
      _params[param] = params[param] || _params[param];
    }
  }
  var imgRep = ic.first();
  var nonSystemImgProps = ee.Feature(null)
    .copyProperties(imgRep).propertyNames();
  if (!_params.bands) _params.bands = imgRep.bandNames();
  if (!_params.bandsRename) _params.bandsRename = _params.bands;
  if (!_params.imgProps) _params.imgProps = nonSystemImgProps;
  if (!_params.imgPropsRename) _params.imgPropsRename = _params.imgProps;
  var results = ic.map(function(img) {
    img = ee.Image(img.select(_params.bands, _params.bandsRename))
      .set(_params.datetimeName, img.date().format(_params.datetimeFormat))
      .set('timestamp', img.get('system:time_start'));
    var propsFrom = ee.List(_params.imgProps)
      .cat(ee.List([_params.datetimeName, 'timestamp']));
    var propsTo = ee.List(_params.imgPropsRename)
      .cat(ee.List([_params.datetimeName, 'timestamp']));
    var imgProps = img.toDictionary(propsFrom).rename(propsFrom, propsTo);
    var fcSub = fc.filterBounds(img.geometry());
    return img.reduceRegions({
      collection: fcSub,
      reducer: _params.reducer,
      scale: _params.scale,
      crs: _params.crs
    })
    .map(function(f) {
      return f.set(imgProps);
    });
  }).flatten().filter(ee.Filter.notNull(_params.bandsRename));
  return results;
}

//Create site buffer for data extraction
var sitebuffer = station.map(bufferPoints(90, false));


/* -------------Import and preprocess covariate datasets-------------*/ 

//Get soil properties
var SoilGridSOC = SoilGrid100.filterMetadata("system:index", "contains", "soc");
var SoilGridBD = SoilGrid100.filterMetadata("system:index", "contains", "bd");
var SoilGridClay = SoilGrid100.filterMetadata("system:index", "contains", "clay");
var SoilGridSand = SoilGrid100.filterMetadata("system:index", "contains", "sand");

//import and calculate basic topographic properties
var elevation = SRTM.clip(sitebuffer).select('elevation').rename('EL');
var slope = ee.Terrain.slope(elevation).rename('SL');
var aspect = ee.Terrain.aspect(elevation).rename('AS');
var twi = (flowaccum.multiply(ee.Image(flowaccum.projection().nominalScale())
            .divide(slope.multiply(ee.Image(Math.PI)).divide(ee.Image(180)).tan()))).log().rename('twi');
var rough = roughness.filterBounds(sitebuffer).mosaic().rename("surfrough");

//Use TAGEE package for calculating curvatures
var TAGEE = require('users/joselucassafanelli/TAGEE:TAGEE-functions');
var gaussianFilter = ee.Kernel.gaussian({
  radius: 3, sigma: 2, units: 'pixels', normalize: true
});
var demSRTM = SRTM.convolve(gaussianFilter).resample("bilinear");
var DEMAttributes = TAGEE.terrainAnalysis(TAGEE, demSRTM, station);
var mcurv = DEMAttributes.select("MeanCurvature").rename("mcurv");
var hcurv = DEMAttributes.select("HorizontalCurvature").rename("hcurv");
var vcurv = DEMAttributes.select("VerticalCurvature").rename("vcurv");

//Get LULC factors
var nlcd = NLCD.select(0); //select the land cover layer
var nlcdlist = ['2001','2004','2006','2008','2011','2013','2016'];
var nlcd = nlcd.filter(ee.Filter.inList('system:index', nlcdlist)); 
var nlcdall = nlcd.merge(ee.ImageCollection([NLCD2019.rename("landcover")
  .setMulti({"system:index": "2019"})]));


/* -------------Get NLCD and tree cover for site-year combination-------------*/

//Get site-year list and use it to extract location and time based LULC and tree cover variables 
var num = site_year.size().getInfo();
var siteyearbuffer = site_year.map(bufferPoints(90, false));
var listofbuffers = siteyearbuffer.toList(siteyearbuffer.size());
var yearlist = site_year.aggregate_array('Water_year');
var nlcdlist = yearlist.replaceAll(2002,"1_2001").replaceAll(2003,"1_2004").replaceAll(2004,"1_2004").replaceAll(2005,"1_2006")
 .replaceAll(2006,"1_2006").replaceAll(2007,"1_2008").replaceAll(2008,"1_2008").replaceAll(2009,"1_2008").replaceAll(2010,"1_2011")
 .replaceAll(2011,"1_2011").replaceAll(2012,"1_2013").replaceAll(2013,"1_2013").replaceAll(2014,"1_2013").replaceAll(2015,"1_2016")
 .replaceAll(2016,"1_2016").replaceAll(2017,"1_2016").replaceAll(2018,"2_2019").replaceAll(2019,"2_2019");

//Define parameters/ variables for extraction
var paramscov = {
  scale: 30,
  crs: 'EPSG:4326',
  reducer: ee.Reducer.median(),
  bands: ['landcover', 'tree'],
  bandsRename: ['LULC', 'Tree']
};

//Main extraction
var listforpred = ee.List.sequence(0, num-1);
var Extresult = listforpred.map(function(i){
   var nlcdsite = nlcdall.filterMetadata("system:index", "equals", nlcdlist.get(i)).first();
   var modistree = MODISTree.filterMetadata("system:index", "contains", ee.String(yearlist.get(i))).first()
   .select("Percent_Tree_Cover").rename("tree");
   var covariates = ee.Image.cat(nlcdsite,modistree)
   .set('system:time_start',ee.Date('2019-01-01').millis());
return covariates;
})
var Covcol = ee.ImageCollection(Extresult);
var listofimages = Covcol.toList(Covcol.size());
var loop = function(i){
 var Cov = ee.ImageCollection([listofimages.get(i)]);
 var buffer = ee.FeatureCollection([listofbuffers.get(i)]);
 return zonalStats(Cov, buffer, paramscov);
}
var result1 = ee.FeatureCollection(listforpred.map(loop)).flatten();

//Export results to csv
Export.table.toDrive({
  collection: result1,
  selectors: ['Site', 'State', 'Water_year', 'LULC', 'Tree'],
  fileFormat: 'CSV',
  });


/* -------------Get soil properties for site-depth combination-------------*/ 

//Get site-depth list and use it to extract location and depth based soil variables 
var num2 = site_depth.size().getInfo();
var sitedepthbuffer = site_depth.map(bufferPoints(90, false));
var listofbuffers = sitedepthbuffer.toList(sitedepthbuffer.size());
var depthlist = site_depth.aggregate_array('Depth');
var soildp1list =  depthlist.replaceAll(5,"d2").replaceAll(10,"d2").replaceAll(20,"d3").replaceAll(50,"d4").replaceAll(51,"d4")
 .replaceAll(100,"d6").replaceAll(102,"d6");
var soildp2list =  depthlist.replaceAll(5,"d2").replaceAll(10,"d3").replaceAll(20,"d4").replaceAll(50,"d5").replaceAll(51,"d5")
 .replaceAll(100,"d6").replaceAll(102,"d6");

//Define parameters/ variables for extraction
var paramscov = {
  scale: 30,
  crs: 'EPSG:4326',
  reducer: ee.Reducer.median(),
  bands: ['soc','bd','clay','sand'],
  bandsRename: ['SOC','BD','Clay','Sand']
};

//Main extraction
var listforpred = ee.List.sequence(0, num2-1);
var Extresult = listforpred.map(function(i){
 var soc1 = SoilGridSOC.filterMetadata("system:index", "contains", soildp1list.get(i)).first();
 var soc2 = SoilGridSOC.filterMetadata("system:index", "contains", soildp2list.get(i)).first();
 var soc = soc1.add(soc2).divide(2).rename("soc"); //Soilgrid+ unit g/kg
 var bd1 = SoilGridBD.filterMetadata("system:index", "contains", soildp1list.get(i)).first();
 var bd2 = SoilGridBD.filterMetadata("system:index", "contains", soildp2list.get(i)).first();
 var bd = bd1.add(bd2).divide(2).divide(1000).rename("bd"); //Soilgrid+ unit convert to g/cm3
 var clay1 = SoilGridClay.filterMetadata("system:index", "contains", soildp1list.get(i)).first();
 var clay2 = SoilGridClay.filterMetadata("system:index", "contains", soildp2list.get(i)).first();
 var clay = clay1.add(clay2).divide(2).rename("clay"); //Soilgrid+ unit %
 var sand1 = SoilGridSand.filterMetadata("system:index", "contains", soildp1list.get(i)).first();
 var sand2 = SoilGridSand.filterMetadata("system:index", "contains", soildp2list.get(i)).first();
 var sand = sand1.add(sand2).divide(2).rename("sand"); //Soilgrid+ unit %
 var covariates = ee.Image.cat(soc,bd,clay,sand)
   .set('system:time_start', ee.Date('2019-01-01').millis());
return covariates;
})
var Covcol = ee.ImageCollection(Extresult);
var listofimages = Covcol.toList(Covcol.size());
var loop = function(i){
 var Cov = ee.ImageCollection([listofimages.get(i)]);
 var buffer = ee.FeatureCollection([listofbuffers.get(i)]);
 return zonalStats(Cov, buffer, paramscov);
}
var result2 = ee.FeatureCollection(listforpred.map(loop)).flatten();

//Export results to csv
Export.table.toDrive({
  collection: result2,
  selectors: ['Site', 'State', 'Depth', 'SOC', 'BD', 'Clay', 'Sand'],
  fileFormat: 'CSV',
  });


/* -------------Get topography for sites-------------*/ 

//Get station/site list and use it to extract location based topographic variables 
var num3 = station.size().getInfo())
var listofbuffers = sitebuffer.toList(sitebuffer.size());

//Define parameters/ variables for extraction
var paramscov = {
  scale: 30,
  crs: 'EPSG:4326',
  reducer: ee.Reducer.median(),
  bands: ['EL','SL','AS','twi','mcurv','hcurv','vcurv','surfrough'],
  bandsRename: ['EL','SL','AS','TWI','mcurv','hcurv','vcurv','surfrough']
};

//Main extraction
var listforpred = ee.List.sequence(0, num3-1);
var Extresult = listforpred.map(function(i){
   var covariates = ee.Image.cat(elevation,slope,aspect,twi,mcurv,hcurv,vcurv,rough)
   .set('system:time_start',ee.Date('2019-01-01').millis());
return covariates;
})
var Covcol = ee.ImageCollection(Extresult);
var listofimages = Covcol.toList(Covcol.size());
var loop = function(i){
 var Cov = ee.ImageCollection([listofimages.get(i)]);
 var buffer = ee.FeatureCollection([listofbuffers.get(i)])
 return zonalStats(Cov, buffer, paramscov)
}
var result3 = ee.FeatureCollection(listforpred.map(loop)).flatten()

//Export results to csv
Export.table.toDrive({
  collection: result3,
  selectors: ['Site', 'State', 'EL', 'SL', 'AS', 'TWI', 'mcurv', 'hcurv', 'vcurv','surfrough'],
  fileFormat: 'CSV',
  });
